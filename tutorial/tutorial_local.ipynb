{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pVhOfzLx9us"
      },
      "source": [
        "#Waymo Open Dataset Tutorial (using local Jupyter kernel)\n",
        "\n",
        "- Website: https://waymo.com/open\n",
        "- GitHub: https://github.com/waymo-research/waymo-open-dataset\n",
        "\n",
        "This tutorial demonstrates how to use the Waymo Open Dataset with two frames of data. Visit the [Waymo Open Dataset Website](https://waymo.com/open) to download the full dataset.\n",
        "\n",
        "To use:\n",
        "1. checkout `waymo_open_dataset` (need to run once):\n",
        "   ```\n",
        "   git clone https://github.com/waymo-research/waymo-open-dataset.git waymo-open-dataset-repo\n",
        "   cd waymo-open-dataset-repo\n",
        "   ```\n",
        "2. build docker container (need to run once):\n",
        "   ```\n",
        "   docker build --tag=open_dataset_jupyter -f tutorial/cpu-jupyter.Dockerfile .\n",
        "   ```\n",
        "3. start Jupyter kernel inside the docker container:\n",
        "   ```\n",
        "   docker run -p 8888:8888 open_dataset_jupyter\n",
        "   ```\n",
        "4. connect the notebook to the local port `8888`.\n",
        "\n",
        "Uncheck the box \"Reset all runtimes before running\" if you run this colab directly from the remote kernel. Alternatively, you can make a copy before trying to run it by following \"File \u003e Save copy in Drive ...\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8pO1b1EALPA"
      },
      "source": [
        "### Metrics computation\n",
        "The core metrics computation library is written in C++, so it can be extended to other programming languages. It can compute detection metrics (mAP) and tracking metrics (MOTA). See more information about the metrics on the [website](https://waymo.com/open/next/).\n",
        "\n",
        "We provide command line tools and TensorFlow ops to call the detection metrics library to compute detection metrics. We will provide a similar wrapper for tracking metrics library in the future. You are welcome to contribute your wrappers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBufmNJBdGR"
      },
      "source": [
        "#### Command line detection metrics computation\n",
        "\n",
        "The command takes a pair of files for prediction and ground truth. Read the comment in waymo_open_dataset/metrics/tools/compute_detection_metrics_main.cc for details of the data format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZ96ro4vp9tj"
      },
      "outputs": [],
      "source": [
        "# copybara removed file resource import\n",
        "import os\n",
        "\n",
        "if os.path.exists('tutorial_local.ipynb'):\n",
        "    # in case it is executed as a Jupyter notebook from the tutorial folder.\n",
        "    os.chdir('../')\n",
        "\n",
        "\n",
        "fake_predictions_path = '{pyglib_resource}waymo_open_dataset/metrics/tools/fake_predictions.bin'.format(pyglib_resource='')\n",
        "fake_ground_truths_path = '{pyglib_resource}waymo_open_dataset/metrics/tools/fake_ground_truths.bin'.format(pyglib_resource='')\n",
        "bin_path = '{pyglib_resource}waymo_open_dataset/metrics/tools/compute_detection_metrics_main'.format(pyglib_resource='')\n",
        "frames_path = '{pyglib_resource}tutorial/frames'.format(pyglib_resource='')\n",
        "point_cloud_path = '{pyglib_resource}tutorial/3d_point_cloud.png'.format(pyglib_resource='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFqlUjAH6V9V"
      },
      "outputs": [],
      "source": [
        "!{bin_path} {fake_predictions_path} {fake_ground_truths_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FpcGDRnAElo"
      },
      "source": [
        "### Load waymo_open_dataset package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwnPUOgVAHec"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "from waymo_open_dataset.utils import frame_utils\n",
        "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
        "\n",
        "tf.enable_eager_execution()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibor0U9XBlX6"
      },
      "source": [
        "## Read one frame\n",
        "\n",
        "Each file in the dataset is a sequence of frames ordered by frame start timestamps. We have extracted two frames from the dataset to demonstrate the dataset format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29uZtYLJBx2r"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.TFRecordDataset(frames_path, compression_type='')\n",
        "for data in dataset:\n",
        "    frame = open_dataset.Frame()\n",
        "    frame.ParseFromString(bytearray(data.numpy()))\n",
        "    break\n",
        "\n",
        "(range_images, camera_projections, _, range_image_top_pose) = (\n",
        "    frame_utils.parse_range_image_and_camera_projection(frame))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eorb6V2qznV_"
      },
      "source": [
        "###Examine frame context\n",
        "\n",
        "Refer to [dataset.proto](https://github.com/waymo-research/waymo-open-dataset/blob/master/waymo_open_dataset/dataset.proto) for the data format. The context contains shared information among all frames in the scene."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZotEevt7S0fE"
      },
      "outputs": [],
      "source": [
        "print(frame.context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjdoulAnU1LK"
      },
      "source": [
        "##Visualize Camera Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne4-TpYLVCwi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(25, 20))\n",
        "\n",
        "def image_show(data, name, layout, cmap=None):\n",
        "  \"\"\"Show an image.\"\"\"\n",
        "  plt.subplot(*layout)\n",
        "  plt.imshow(tf.image.decode_jpeg(data), cmap=cmap)\n",
        "  plt.title(name)\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "\n",
        "for index, image in enumerate(frame.images):\n",
        "  image_show(image.image, open_dataset.CameraName.Name.Name(image.name),\n",
        "             [3, 3, index+1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPc-xBE6VMHi"
      },
      "source": [
        "##Visualize Range Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwZ4xcsHVO1V"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(64, 20))\n",
        "def plot_range_image_helper(data, name, layout, vmin = 0, vmax=1, cmap='gray'):\n",
        "  \"\"\"Plots range image.\n",
        "\n",
        "  Args:\n",
        "    data: range image data\n",
        "    name: the image title\n",
        "    layout: plt layout\n",
        "    vmin: minimum value of the passed data\n",
        "    vmax: maximum value of the passed data\n",
        "    cmap: color map\n",
        "  \"\"\"\n",
        "  plt.subplot(*layout)\n",
        "  plt.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax)\n",
        "  plt.title(name)\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')\n",
        "\n",
        "def get_range_image(laser_name, return_index):\n",
        "  \"\"\"Returns range image given a laser name and its return index.\"\"\"\n",
        "  return range_images[laser_name][return_index]\n",
        "\n",
        "def show_range_image(range_image, layout_index_start = 1):\n",
        "  \"\"\"Shows range image.\n",
        "\n",
        "  Args:\n",
        "    range_image: the range image data from a given lidar of type MatrixFloat.\n",
        "    layout_index_start: layout offset\n",
        "  \"\"\"\n",
        "  range_image_tensor = tf.convert_to_tensor(range_image.data)\n",
        "  range_image_tensor = tf.reshape(range_image_tensor, range_image.shape.dims)\n",
        "  lidar_image_mask = tf.greater_equal(range_image_tensor, 0)\n",
        "  range_image_tensor = tf.where(lidar_image_mask, range_image_tensor,\n",
        "                                tf.ones_like(range_image_tensor) * 1e10)\n",
        "  range_image_range = range_image_tensor[...,0] \n",
        "  range_image_intensity = range_image_tensor[...,1]\n",
        "  range_image_elongation = range_image_tensor[...,2]\n",
        "  plot_range_image_helper(range_image_range.numpy(), 'range',\n",
        "                   [8, 1, layout_index_start], vmax=75, cmap='gray')\n",
        "  plot_range_image_helper(range_image_intensity.numpy(), 'intensity',\n",
        "                   [8, 1, layout_index_start + 1], vmax=1.5, cmap='gray')\n",
        "  plot_range_image_helper(range_image_elongation.numpy(), 'elongation',\n",
        "                   [8, 1, layout_index_start + 2], vmax=1.5, cmap='gray')\n",
        "frame.lasers.sort(key=lambda laser: laser.name)\n",
        "show_range_image(get_range_image(open_dataset.LaserName.TOP, 0), 1)\n",
        "show_range_image(get_range_image(open_dataset.LaserName.TOP, 1), 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17Lwd2nVpex7"
      },
      "source": [
        "##Point Cloud Conversion and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIEDW1pfpmd-"
      },
      "outputs": [],
      "source": [
        "points, cp_points = frame_utils.convert_range_image_to_point_cloud(frame,\n",
        "                                                       range_images,\n",
        "                                                       camera_projections,\n",
        "                                                       range_image_top_pose)\n",
        "points_ri2, cp_points_ri2 = frame_utils.convert_range_image_to_point_cloud(\n",
        "    frame,\n",
        "    range_images,\n",
        "    camera_projections,\n",
        "    range_image_top_pose,\n",
        "    ri_index=1)\n",
        "\n",
        "# 3d points in vehicle frame.\n",
        "points_all = np.concatenate(points, axis=0)\n",
        "points_all_ri2 = np.concatenate(points_ri2, axis=0)\n",
        "# camera projection corresponding to each point.\n",
        "cp_points_all = np.concatenate(cp_points, axis=0)\n",
        "cp_points_all_ri2 = np.concatenate(cp_points_ri2, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MAKwTRWz3af"
      },
      "source": [
        "###Examine number of points in each lidar sensor."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5_e4BCAGfYX"
      },
      "source": [
        "First return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpsAJp2CqKrE"
      },
      "outputs": [],
      "source": [
        "print(points_all.shape)\n",
        "print(cp_points_all.shape)\n",
        "print(points_all[0:2])\n",
        "for i in range(5):\n",
        "  print(points[i].shape)\n",
        "  print(cp_points[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3cdlVYFGiE_"
      },
      "source": [
        "Second return."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX6aj2EDGlRo"
      },
      "outputs": [],
      "source": [
        "print(points_all_ri2.shape)\n",
        "print(cp_points_all_ri2.shape)\n",
        "print(points_all_ri2[0:2])\n",
        "for i in range(5):\n",
        "  print(points_ri2[i].shape)\n",
        "  print(cp_points_ri2[i].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCj3SDbuq9Nr"
      },
      "source": [
        "###Show point cloud\n",
        "3D point clouds are rendered using an internal tool, which is unfortunately not publicly available yet. Here is an example of what they look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8VFnGnOq6cO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(point_cloud_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiGOSt4mr0xA"
      },
      "source": [
        "##Visualize Camera Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRMUN-hur9wO"
      },
      "outputs": [],
      "source": [
        "images = sorted(frame.images, key=lambda i:i.name)\n",
        "cp_points_all_concat = np.concatenate([cp_points_all, points_all], axis=-1)\n",
        "cp_points_all_concat_tensor = tf.constant(cp_points_all_concat)\n",
        "\n",
        "# The distance between lidar points and vehicle frame origin.\n",
        "points_all_tensor = tf.norm(points_all, axis=-1, keepdims=True)\n",
        "cp_points_all_tensor = tf.constant(cp_points_all, dtype=tf.int32)\n",
        "\n",
        "mask = tf.equal(cp_points_all_tensor[..., 0], images[0].name)\n",
        "\n",
        "cp_points_all_tensor = tf.cast(tf.gather_nd(\n",
        "    cp_points_all_tensor, tf.where(mask)), dtype=tf.float32)\n",
        "points_all_tensor = tf.gather_nd(points_all_tensor, tf.where(mask))\n",
        "\n",
        "projected_points_all_from_raw_data = tf.concat(\n",
        "    [cp_points_all_tensor[..., 1:3], points_all_tensor], axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Invsxz0xsXNA"
      },
      "outputs": [],
      "source": [
        "def rgba(r):\n",
        "  \"\"\"Generates a color based on range.\n",
        "\n",
        "  Args:\n",
        "    r: the range value of a given point.\n",
        "  Returns:\n",
        "    The color for a given range\n",
        "  \"\"\"\n",
        "  c = plt.get_cmap('jet')((r % 20.0) / 20.0)\n",
        "  c = list(c)\n",
        "  c[-1] = 0.5  # alpha\n",
        "  return c\n",
        "\n",
        "def plot_image(camera_image):\n",
        "  \"\"\"Plot a cmaera image.\"\"\"\n",
        "  plt.figure(figsize=(20, 12))\n",
        "  plt.imshow(tf.image.decode_jpeg(camera_image.image))\n",
        "  plt.grid(\"off\")\n",
        "\n",
        "def plot_points_on_image(projected_points, camera_image, rgba_func,\n",
        "                         point_size=5.0):\n",
        "  \"\"\"Plots points on a camera image.\n",
        "\n",
        "  Args:\n",
        "    projected_points: [N, 3] numpy array. The inner dims are\n",
        "      [camera_x, camera_y, range].\n",
        "    camera_image: jpeg encoded camera image.\n",
        "    rgba_func: a function that generates a color from a range value.\n",
        "    point_size: the point size.\n",
        "\n",
        "  \"\"\"\n",
        "  plot_image(camera_image)\n",
        "\n",
        "  xs = []\n",
        "  ys = []\n",
        "  colors = []\n",
        "\n",
        "  for point in projected_points:\n",
        "    xs.append(point[0])  # width, col\n",
        "    ys.append(point[1])  # height, row\n",
        "    colors.append(rgba_func(point[2]))\n",
        "\n",
        "  plt.scatter(xs, ys, c=colors, s=point_size, edgecolors=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fx7mUQM2saI-"
      },
      "outputs": [],
      "source": [
        "plot_points_on_image(projected_points_all_from_raw_data,\n",
        "                     images[0], rgba, point_size=5.0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "",
        "kind": "local"
      },
      "name": "Waymo Open Dataset Tutorial (using local Jupyter kernel).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
